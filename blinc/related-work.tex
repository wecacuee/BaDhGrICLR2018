\paragraph{Localization and mapping}
Robotic localization and mapping for navigation as a problem since the beginning of mobile robotics and sensing.
Smith and Cheeseman~\cite{SmChIJRR1986} introduced the idea of propagating spatial uncertainty for robot localization while mapping and Elfes popularized Occupancy Grids~\cite{ElCOMPUTER1980} for mapping.
In the last three decades, the field has exploded with variation of algorithms for different sensors like cameras, laser scanners, sonars, depth sensors, variation in level of detail like topological maps \cite{KuCOGSCI1978} for low level of detail to occupancy grid maps for high detail and variation in environment types like highly textured or non-textured.

All these approaches require huge amount of hand-tuning and design for adapting to different environments and sensor types. The level of detail of maps also needs to be decided before hand irrespective of the application and hence is not optimized for the application at hand.

\paragraph{Deep reinforcement learning}
Deep reinforcement learning (DRL) came back to the limelight \cite{TeACM1995} \hl{Check whether this citation should be here} with Mnih \etal~ \cite{MnKaSiNATURE2015,MnKaSiNIPSDLW2013} demonstrating that their algorithms outperform humans on Atari games. Subsequently, the DRL algorithms have been extended \cite{MnBaMiICML2016} and applied to various games \cite{SiHuMaNATURE2016}, simulated platforms \cite{KaStJoNIPS2017}, real world robots \cite{LePaKrISER2017} and more recently to robotic navigation \cite{MiPaViICLR2017,OhChSiICML2016}.

The exploration into robotic navigation using deep reinforcement learning is a nascent topic, it has potential to disrupt the fields of simultaneous localization and mapping and path planning. Also, \cite{MiPaViICLR2017} train and test on the same maps which limits our understanding of the generality of the method. In fact, it is very common to train and test on the same environments in reinforcement learning based navigation works \cite{ZhMoKoICRA2017,KuSaGaAPA2016} with the only variation being in location of goal and starting point. In contrast, \cite{OhChSiICML2016} do test on random maps but the only decision that the agent has to make is avoid a goal of particular color and seek other color rather than remembering the path to the goal. On similar lines, \cite{ChLaSaNIPS2016} test their method on unseen maps in VizDoom environment but only vary the maps by unseen texture.
%
In this work, we take the study of these methods significantly farther with a thorough investigation of whether DRL-based agents remember enough information to obviate mapping algorithms or the need to be augmented with mapping algorithms.


\paragraph{Inattention blindness in humans}
Our experiments with blindfolding can be interpreted in relationship with attention affordance while driving. How much inattention can we afford while driving on a particular speed? While these questions haven't been answered in psychology, there have been studies \cite{IrBoWiACP2010} showing that cell phone users driver more slowly and follow farther from a paced car indicating that visual cues for navigation are dependent the distance travelled rather than the time travelled. But the very idea that cell phone users can drive supports the idea that not all visual information at all frames is not necessary for navigation.

While humans do not literally blindfold themselves while driving, they do suffer from inattentional blindness. Inattentional blindness is more common in familiar scenarios and experienced drivers then in unfamiliar scenarios \cite{YaLiFeAAAI2015,YaSpHF2014,ChStTPB2013}.

Apart from the similarities, our experiments differ significantly from psychology experiments that explore the relationship between inattentional blindness and route-familiarity. While the psychology experiments measure change in reaction time to emergency events in dynamic environments, we measure the percent of time agent can afford blindfolding while navigating static familiar route.

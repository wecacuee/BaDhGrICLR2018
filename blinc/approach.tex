% 0. Describe baseline models
% 1. Introduce blinding in a curriculum and finetuning manner
% 2. Introduce the concept of doubling the action space
% 3. Introduce the potential applicablity of this blinding to querying
% the surrounding states of the agent.

Our agents are trained on the same environments utilized in the Mirowski \etal paper. We additionally generate a variety of random mazes of dimensions 7x7, 9x9, 11x11, 13x13 and 15x15 and quantify performance on them as well.

\subsection{Baseline Models}
We utilize the model presented in Mirowski \etal as our baseline model. The agent's egocentric view is fed in to a deep network that in turn converts it to an action to take within the environment. Due to the POMDP nature of the problem, memory is provided to the agent via a set of stacked LSTMs so that it can learn to assign contextual importances to past actions and observations. As in the original work, we provide our agents will the auxiliary loss signals of depth prediction and loop closure to improve convergence speed and stability. Our agents utilize a simpler, discretized motion model wherein at every point they must choose between the forward, backward, rotate left and rotate right actions.

\subsection{Blinding}
We incrementally blind these agents so as to guage the amount of information that is actually required by agents to perform navigation in the environments on which they have been trained. Intuitively, we expect agents trained with some form of blinding to perform better long-term planning due to blindness-related unreliability in the expected future observations. We experiment with two kinds of blinding: curriculum and fine-tuning. 

\subsubsection{Curriculum Blinding}
In currulum blinding, agents are incrementally blinded over training time while simulateneously being tasked with the learning to navigate task. The agent is tasked with learning a harder combined task in the hope that it more readily learns how its actions correspond to rewards gained in the long term. Blinding is linearly increased from 0\% to 100\% over the course of each trial. 

\subsubsection{Fine-tuning}
In the fine-tuning approach, baseline agents are first pre-trained on the maps using the standard A3C approach. Agents are then "fine-tuned" by being trained on the incrementally blind data where the blindness  again increases from 0\% to 100\% over the course of training. These section of the experiments test whether the agents can transform their learned short term navigational plans to more long term ones and are also useful in determining where failiure occurs.

\subsection{Blinding: Self-Supervised Blinding-Curriculum Training}
In the previous experiments, blinding is forced upon the agent at training time. The agent at each instant is expected to formulate some sort of plan for future navigation due to the expected unreliabilities in future observations. Navigating blind, however, varies in difficulty greatly depending on the geometery of the section of the maze surrounding the agent at any point. For example, navigating blind down a straight corridor is a conceptually easier task than navigating a hallway with several turns in it. 

Based on this intuition, we introduce our BLINC method. In BLINC


Every maze, however, possesses different degrees of difficulty in their different parts. For example, navigating a single corridor blind is a much easier task than that of navigating cross roads.

Based on this idea, we introduce BLINC, wherein agents are incentivized via small additional reward signals to 

% 0. Describe baseline models
% 1. Introduce blinding in a curriculum and finetuning manner
% 2. Introduce the concept of doubling the action space
% 3. Introduce the potential applicablity of this blinding to querying
% the surrounding states of the agent.

Our agents are trained on the same environments utilized in the Mirowski \etal paper. We additionally generate a variety of random mazes of dimensions 7x7, 9x9, 11x11, 13x13 and 15x15 and quantify performance on them as well.

\subsection{Baseline Models}
We utilize the model presented in Mirowski \etal as our baseline model. The agent's egocentric view is fed in to a deep network that in turn converts it to an action to take within the environment. Due to the POMDP nature of the problem, memory is provided to the agent via a set of stacked LSTMs so that it can learn to assign contextual importances to past actions and observations. As in the original work, we provide our agents will the auxiliary loss signals of depth prediction and loop closure to improve convergence speed and stability. Our agents utilize a simpler, discretized motion model wherein at every point they must choose between the \emph{forward}, \emph{backward}, \emph{rotate left} and \emph{rotate right} actions.

\subsection{Blinding}
We incrementally blind these agents so as to guage the amount of information that is actually required by agents to perform navigation in the environments on which they have been trained. Intuitively, we expect agents trained with some form of blinding to perform better long-term planning due to blindness-related unreliability in the expected future observations. We experiment with two kinds of blinding: curriculum and fine-tuning. 

\subsubsection{Curriculum Blinding}
In currulum blinding, agents are incrementally blinded over training time while simulateneously being tasked with the learning to navigate task. The agent is tasked with learning a harder combined task in the hope that it more readily learns how its actions correspond to rewards gained in the long term. Blinding is linearly increased from 0\% to 100\% over the course of each trial. 

\subsubsection{Fine-tuning}
In the fine-tuning approach, baseline agents are first pre-trained on the maps using the standard A3C approach. Agents are then "fine-tuned" by being trained on the incrementally blind data where the blindness  again increases from 0\% to 100\% over the course of training. These section of the experiments test whether the agents can transform their learned short term navigational plans to more long term ones and are also useful in determining where failiure occurs.

\subsection{Blinding: Self-Supervised Blinding-Curriculum Training}
In the previous experiments, blinding is forced upon the agent at training time. The agent at each instant is expected to formulate some sort of plan for future navigation due to the expected unreliabilities in future observations. Navigating blind, however, varies in difficulty greatly depending on the geometery of the section of the maze surrounding the agent at any point. For example, navigating blind down a straight corridor is a conceptually easier task than navigating a hallway with several turns in it. 

Based on this intuition, we introduce the BLINC method. In BLINC, agents are incentivized via small rewards to blind themselves during navigation. This blinding is optional and is introduced via a doubling of the state space. At every instant, the agent must choose between the \emph{forward}, \emph{blind-forward}, \emph{backward}, \emph{blind-backward}, \emph{rotate left}, \emph{blind-rotate left}, \emph{rotate right} and \emph{blind-rotate right} actions. When blind actions are chosen, the agent recieves small positive reward values so as to encourage it perform this actions often. Similar to previous experiments, the blinding is introduced in incremental fashion so that the agent may first learn how blind-actions correspond to the non-blind ones in terms of action performed. We experiment with different reward structures to incentivize self-blinding as often as possible. In particular, we are interested in agents that choose to blind themselves contiguously over several frames so we may conclude that some form of long-term planning is occuring. 

\subsubsection{Constant Reward}
In the constant-reward version of the experiment, agents are rewarded with a constant small reward for every blind action taken. Care is taken to ensure that the cumulative reward of performing ab lind action at every frame is less than that of find the goal in the maze once.

\subsubsection{Contiguous-Linear Reward}
In the contiguous-linear case, the first blind action is rewarded with a small position value. For every subsquent blind action performed immediately afterwards, the reward is linearly increased by a the number of blind frames till that point. The reward at the $n^{th}$ contiguous blind frame is:

\center{$r_n = \frac{R}{2*F} + \frac{R}{(2*F)(2*(F+1))}$}

\subsubsection{Contiguous-Quadratic Reward}
Some contiguous reward structure that solves ALL our problems. WHERE ARE YOU!

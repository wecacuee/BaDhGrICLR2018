\documentclass{article} % For LaTeX2e
\input{preamble}
\title{Do deep reinforcement learning algorithms really learn to navigate?}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Shurjo Banerjee\footnotemark[1],%
  \, Vikas Dhiman\thanks{indicates equal contribution},%
  \, Brent Griffin,%
  \, \& Jason J. Corso\\
  The Electrical Engineering and Computer Science Deparment\\
The University of Michigan\\
Ann Arbor, MI 48109, USA \\
\texttt{\{shurjo,dhiman,griffb,jjcorso\}@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
  Deep reinforcement learning (DRL) algorithms have demonstrated strong progress in learning to reach a goal in challenging environments.
  As the title of the paper by \cite{MiPaViICLR2017} suggests, it seems that Deep reinforcement learning is able to learn to navigate and is thus ready to replace traditional mapping and path planning algorithms.
  Yet, from experiments and analysis in this earlier work, it is not clear whether or not these DRL agents are indeed doing any form of mapping and path planning.
  In this paper, we pose and study this question: are DRL agents doing some form of mapping or path-planning?  Our experiments show that the agents are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.
  Hence, the DRL agents fall short of qualifying as mapping and path-planning agents.
  We find evidence that the DRL agents are learning a correspondence to perform an action for each frame and there is no evidence of path-planning.
\end{abstract}

\section{Introduction}
%\input{intro-drl-doesnt-generalize}
\input{intro-drl-not-path-planning}
\section{Related Work}
\input{related-work}

\section{Background}
\input{background}

%\section{Approach}
%\input{approach}
\section{Experiments}
\section{Navigational tasks with increasing difficulty}
\label{sec:navtasks}
\input{intro-drl-nav-challenge}

\input{experiments}

%\section{Analysis}
%\input{analysis}

\section{Conclusion}
\input{conclusion}

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\IfFileExists{/z/home/dhiman/wrk/group-bib/shared.bib}{
  \bibliography{/z/home/dhiman/wrk/group-bib/shared,main,main_filtered}
}{
  \bibliography{main,main_filtered}
}
\bibliographystyle{iclr2018_conference}

\end{document}

\documentclass{article} % For LaTeX2e
\input{preamble}
\title{Navigating Unseen Environments using Deep Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Shurjo Banerjee\footnotemark[1],%
  \, Vikas Dhiman\thanks{indicates equal contribution},%
  \, Brent Griffin,%
  \, \& Jason J. Corso\\
  The Electrical Engineering and Computer Science Deparment\\
The University of Michigan\\
Ann Arbor, MI 48109, USA \\
\texttt{\{shurjo,dhiman,griffb,jjcorso\}@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
In recent years, agents trained via Deep Reinforcement Learning based methods have shown a lot of promise in learning to navigate to goal locations in complicated three-dimensional worlds. Of paritcular interest has been the fact that these agents have to shown to reliably perform this navigation  without the need for any explicit SLAM or path-planning. In their current form, however, these agents are not well understood enough to serve as a viable replacement to traditional methods. Of particular concern has been the literature's standard approach of utilizing the same environments for training and testing in the evaluation of these agents. In this work we (a) Reimplement state-of-the-art DRL agents and quantify their performance across thousands of seen and unseen environments possessing large amounts of variety (b) We introduce simple modifications to standard architectures to improve the performance of these agents on previously unseen worlds. Our findings showcase that state-of-art methods are able to generlized learned behaviors but retain no memory of previously visited areas. Our modified agents are found to outperform them drastically via the explicit use of frame-action look up tables within these networks. 
\end{abstract}

\section{Introduction}
%\input{intro-drl-doesnt-generalize}
\input{intro-drl-not-path-planning}
\input{intro-drl-nav-challenge}

\section{Related Work}
\input{related-work}

\section{Background}
\input{background}

\section{Approach}
\input{approach}

\section{Experiments}
\input{experiments}

\section{Analysis}
\input{analysis}

\section{Conclusion}
\input{conclusion}


\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\IfFileExists{/z/home/dhiman/wrk/group-bib/shared.bib}{
  \bibliography{/z/home/dhiman/wrk/group-bib/shared,main,main_filtered}
}{
  \bibliography{main,main_filtered}
}
\bibliographystyle{iclr2018_conference}

\end{document}

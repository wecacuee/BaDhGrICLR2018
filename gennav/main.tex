\documentclass{article} % For LaTeX2e
\input{preamble}
\title{Do deep reinforcement learning algorithms really learn to navigate?}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Shurjo Banerjee\footnotemark[1],%
  \, Vikas Dhiman\thanks{indicates equal contribution},%
  \, Brent Griffin,%
  \, \& Jason J. Corso\\
  The Electrical Engineering and Computer Science Deparment\\
The University of Michigan\\
Ann Arbor, MI 48109, USA \\
\texttt{\{shurjo,dhiman,griffb,jjcorso\}@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

% Document Outline
% - Summary and marketing
% - Short tutorial on Deep reinforcement learning upto Nav-A3C
% - Describe the setup:
%    + What is the game
%       * What are the constants
%       * What are the variables that will be varied
%    + Reward design
% - How the setup is varied for various arguments
% - What are the questions that we want to answer?
% - Why do we do the experiments that we do?
% - What do the experiments tell us?
% - Summary
\begin{document}
\maketitle
\begin{abstract}
  Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments.
  As the title of the paper by \cite{MiPaViICLR2017} suggests, one might assume that DRL based algorithms are able to ``learn to navigate'' and are thus ready to replace classical mapping and path planning algorithms, at least in simulated environments.
  Yet, from experiments and analysis in this earlier work, it is not clear what strategies these DRL algorithms are learning to navigate the mazes and find the goal.
  In this paper, we pose and study this question: are DRL algorithms doing some form of mapping and/or path-planning?  Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.
  Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping.
  We extend the experiments in \cite{MiPaViICLR2017} by separating the set of training and testing maps and by a more systemetic coverage of the space of experiment.
  Our experiments show that the \NavAiiiCDiDiiL{} algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal.
  However, when tested on unseen maps the algorithms utilize a bug exploration like strategy to find the goal without doing any mapping or path planning.
\end{abstract}

\section{Introduction}
%\input{intro-drl-doesnt-generalize}
\input{intro-drl-not-path-planning}

\section{Related Work}
\input{related-work}

\section{Background}
\input{background}

%\section{Approach}
%\input{approach}
\section{The DRL Navigation Challenge}
\input{experiments}

\section{Results and Analysis}
\label{sec:analysis}
\input{analysis}

\section{Conclusion}
\input{conclusion}

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

{\small
\IfFileExists{/z/home/dhiman/wrk/group-bib/shared.bib}{
  \bibliography{/z/home/dhiman/wrk/group-bib/shared,main,main_filtered}
}{
  \bibliography{main,main_filtered}
}
\bibliographystyle{iclr2018_conference}
}

\end{document}

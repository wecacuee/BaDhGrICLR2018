\subsection{Experiments}
\label{sec:navtasks}
We evaluate the \NavAiiiCDiDiiL{} algorithm on maps with 5 stages of difficulty. While the algorithm works smoothly on the easier stages, it does not perform better than wall-following  methods on the hardest stage.
We propose these experiments as a 5-stage benchmark for all end-to-end navigation algorithms.

\begin{description}
  \ditem{Static goal, static spawn, and static map}
  \label{prob:sss}
  To perform optimally on this experiment, the agent needs to find and learn the shortest path at training time and repeat it during testing. 

  \ditem{Static goal, random spawn and static map}
  This is a textbook version of the reinforcement learning problem, especially in grid-world \cite{SuBaBOOK1998}, with the only difference being that the environment is partially observable instead of fully observable.
  This problem is more difficult than Problem~\ref{prob:sss} because the agent
  must find an optimal policy to the goal from each possible starting point in the maze.
  \ditem{Random goal, static spawn, and static map}
  In this setup, we keep the spawn location and the map fixed during both training and testing but choose a random goal location for each episode.
  Note that the goal location stays constant throughout an episode.
  The agent can perform well on this experiment by remembering the goal location after it has been discovered and exploiting the information to revisit the goal faster.  
  
  \ditem{Random goal, random spawn, and static map}
  In this version of the experiment both the spawn point and the goal location is randomized. To perform optimally, the agent must localize itself within the map in addition to being able to exploit map-information.
  
  This is the problem that is addressed by \cite{MiPaViICLR2017} with limited success. 
  They evaluate this case on two maps and report \LatencyOneGtOne{} to be greater than 1 in one of the two maps. We evaluate the same metric on ten other maps.
  \ditem{Random goal, random spawn, and random map}
    We believe that any proposed algorithms on end-to-end navigation problems, should be evaluated on unseen maps.
    To our knowledge, this is the first paper to do so in the case of deep reinforcement learning based navigation methods.
    We train agents to simultaneously learn to explore 1, 10, 100, 500 and 1000 maps and test them on the same 100 unseen maps. The relevant results can be found in Fig~\ref{fig:num-training-maps} and discussed in Section~\ref{sec:analysis}. 
\end{description}

The comparative evaluation of the different the stages of this benchmark are shown in Fig~\ref{fig:latency-goal-reward} and expanded upon in the Section~\ref{sec:analysis}.



\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\input{preamble}
\title{Do deep reinforcement learning algorithms really learn to navigate?}

\author{Shurjo Banerjee\footnotemark[1],%
  \, Vikas Dhiman\thanks{indicates equal contribution},%
  \, Brent Griffin,%
  \, \& Jason J. Corso\\
  The Electrical Engineering and Computer Science Deparment\\
The University of Michigan\\
Ann Arbor, MI 48109, USA \\
\texttt{\{shurjo,dhiman,griffb,jjcorso\}@umich.edu} \\
}

\begin{document}
\maketitle
\begin{abstract}
  Deep reinforcement learning (DRL) algorithms have demonstrated progress in learning to find a goal in challenging environments.
  As the title of the paper by \cite{MiPaViICLR2017} suggests, one might assume that DRL-based algorithms are able to ``learn to navigate'' and are thus ready to replace classical mapping and path-planning algorithms, at least in simulated environments.
  Yet, from experiments and analysis in this earlier work, it is not clear what strategies are used by these algorithms in navigating the mazes and finding the goal.
  In this paper, we pose and study this underlying question: are DRL algorithms doing some form of mapping and/or path-planning?  Our experiments show that the algorithms are not memorizing the maps of mazes at the testing stage but, rather, at the training stage.
  Hence, the DRL algorithms fall short of qualifying as mapping or path-planning algorithms with any reasonable definition of mapping.
  We extend the experiments in \cite{MiPaViICLR2017} by separating the set of training and testing maps and by a more ablative coverage of the space of experiments.
  Our systematic experiments show that the \NavAiiiCDiDiiL{} algorithm, when trained and tested on the same maps, is able to choose the shorter paths to the goal. % GRIFFB: shortest instead of shorter? VD: shorter is right.
  However, when tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning.
\end{abstract}

\section{Introduction}
%\input{intro-drl-doesnt-generalize}
\input{intro-drl-not-path-planning}

\section{Related Work}
\input{related-work}

\section{Background}
\input{background}

\section{The DRL Navigation Challenge}
\input{experiments}

\section{Results and Analysis}
\label{sec:analysis}
\input{analysis}

\section{Conclusion}
\input{conclusion}

% \subsubsection*{Acknowledgments}
% 
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

{\small
\IfFileExists{/z/home/dhiman/wrk/group-bib/shared.bib}{
  \bibliography{/z/home/dhiman/wrk/group-bib/shared,main,main_filtered}
}{
  \bibliography{main,main_filtered}
}
\bibliographystyle{iclr2018_conference}
}


\newpage
\input{appendix}

\end{document}

In this work, we comprehensively evaluate  \NavAiiiCDiDiiL{} (\cite{MiPaViICLR2017}), a DRL-based navigation algorithms, through systematic set of experiments that
are repeated over multiple randomly chosen maps.
Our experiments show that DRL-based navigation models are able to perform some degree of path-planning and mapping when trained and tested on the same map even when spawn locations and goal locations are randomized. 
However the large variation in the evaluation metrics show that how such behaviour is not consistent across episodes. 
We also train and test these methods on disjoint set of maps and show that such trained models fail to perform any form of path-planning or mapping in unseen environments.

In this work, we begin by asking: do DRL-based navigation algorithms really ``learn to navigate''?
Our results answer this question negatively. At best, we can say that DRL-based algorithms learn to navigate in the exact same environment, rather than general technique of navigation which is what classical mapping and path planning provide.
We hope that the systematic approach to the experiments in this work serve as a benchmark for future DRL-based navigation methods.


\subsection{Setup}
Since deep reinforcement learning algorithms need millions of iterations to train, in the absence of thousands of robotic replicas like \cite{LePaKrISER2017}, we evaluate the algorithms on a simulated environment.
We use the same game engine as used by \cite{MiPaViICLR2017}, called Deepmind Lab \cite{BeLeTeARXIV2016}.
The game is setup such that an agent is placed within a randomly generated maze containing a \emph{goal} at a particular location.
On reaching the goal, the agent \emph{respawns} within the same maze while the goal location remains unchanged. 
Following standard conventions we scatter the maze with smaller apple rewards (+1) to encourage initial explorations and assign the goal a +10 reward.
The agents are tasked with maximizing possible reward over the course of episodes of fixed time lengths and thus must learn to repeatedly find the goal within the maze as often as possible to achieve this objective.

Unlike \cite{MiPaViICLR2017}, we include a small wall penalty (-0.2) that pushes the agent away from the wall. We find that this penalty is useful to prevent agents from picking up scattered rewards while moving backwards that in turn lead to local-optima based backward-moving agents that discard vision all together in their explorations.

% Reward formulation
%The setup of game is such that it enables evaluating the algorithm for optimal path planning as well as enable dividing experiments into exploration and exploitation stages.
%We introduce a metric that evaluates how well the agent exploits the information gained during exploration before finding the goal for the first time.
% The optimal way of finding the goal in fastest way is to collect
% enough information for creating a map and use the map to find the shortest path from start to end location.
% However, an end-to-end navigation algorithm might find other ways of finding optimal strategies to find the goal.

Instead of the 8-action continous motion model utilised by \cite{MiPaViICLR2017} we use a discrete 4-action space (move forward/backward, rotate left/right). We do this to speed up training times and disable strafing. The agent's picking up of rewards is thus explicitly ground in vision as the agent must rotate in the direction of motion during exploration.

We generate 1100 random maps using standard depth first search based maze generation methods. More information on maze generation can be found in the appendix. 
Of these 1100 maps, 1000 maps are used for training while the remaining 100 are used for testing.
Unlike \cite{MiPaViICLR2017} and similar to \cite{ChLaSaNIPS2016}, we use randomly textured walls in our mazes so that the policies learned are texture-independent.

Througout our experiments we refer to goal locations, spawn locations and maps as static or random.
When static is used, spawn and goal locations thus described remain fixed through all episodes for both the training and the testing.
When random is used, the spawn location changes randomly throughout the duration of the episode for both training and testing. When used for the goal, random means that its locations changes between episodes for training and testing. During the course of an episode, however, a random goal remains in the same position allowing for agents that exploit map information to find it repeatedly and thus maximize reward.
In the context of maps, statics maps imply that the same map is used for training and testing. Random means that agents are simulataneously trained on multiple maps and tested on previously unseen ones.

\input{grid-2d-dmlab-figure}

\input{intro-drl-nav-challenge}

We evaluate the Nav-A3C\cite{MiPaViICLR2017} algorithm on randomly chosen ten maps with increasing difficulty.
The results of our experiments are shown in Fig~\ref{fig:latency-goal-reward}.
We change the randomness of either the spawn point or the goal location.
We note that the increasing randomness gradually reduce the maximum reward achived and the number of times the goal is reached. As expected, the variance in reward and number of times goals is reached also increases with randomless.

\begin{figure}%
  \includegraphics[width=\linewidth]{images/plot_summary_bar_plots.pdf}%
  \vspace{-1em}%
  \caption{We evaluate the Nav-A3C\cite{MiPaViICLR2017} algorithm on randomly chosen ten maps with increasing difficulty as described in Sec~\ref{sec:navtasks}.
  Vertical axes is one of the ten maps on which the agent was trained and evaluated.
  Horizontal axes are different evaluation metrics.
  The abbreviations in legend are as follows: ``St'' stands for
  static value throughout the training and testing cycle, while ``Rnd'' means random value. ``G'', ``S'' and ``M'' stand for goal, spawn location and map respectively.
  We note that when the goal is static then rewards are consitently higher as compared to random goal while static spawn location and random spawn location are roughly close to each other within bounds of uncertainity. As expected, switching each variable from static to random increases the standard deviation on the results.
  From the \LatencyOneGtOne{} results we note that the current state of art algorithms do well when trained and tested on the same map but fail to generalize to new maps when evaluated on ability to exploit the information about goal location.}%
\label{fig:latency-goal-reward}%
\end{figure}


\subsection{Evaluation Metrics}
We report three evaluation metrics in all our experiments: reward, average number of goal hits and \LatencyOneGtOne{}. We run evaluations on a map for 100 episodes and take the mean and standard deviation of the rewards per episode and number of goal hits per episode. Even though reward includes rewards due to apples and negative rewards due to wall penalities, the reward due to goal dominates dominates the reward metric making reward approximately ten times the average goal hits.

The \LatencyOneGtOne{} is defined as the ratio of time take to find the goal first time to the average time take to find goal thereafter.
Note that whenever the goal location is ``Random'' (different in each episode but same for the episode), untill the goal is found for the first time, the agent is just exploring the map.
After the first goal hit, the agent should be able to exploit the location of the goal to find the goal in shorter times.
Hence a \LatencyOneGtOne{} value greater than one indicates that the algorithm is able to successfully exploit information gathered during goal finding exploration.


\subsection{Effect of apples and texture}
We evaluate the effect of apples and texture during evaluation time in Fig~\ref{fig:num-training-maps}.
We train the algorithm on randomly chosen training maps with random texture and evaluate them no maps with and without random texture and also on maps with and without apples. When the apples are present, we place the apples with probability 0.25 in each block of the map.
We find that the algorithm, being trained on random textures and random placement of apples, is robust to presence or absence of textures and apples.


\subsection{Qualitative evaluation on simple maps}
% 1. We test the trained algorithms on simple maps.
To evaluate what strategies that algorithm is employing to reach the
goal we evaluate the algorithm on very simple maps where there are only two
paths to reach the goal. The qualitative results for the evaluation are shown
in Fig~\ref{fig:planning-qualitative}.

\subsubsection{Square map}
% 3. In the square map, we find that the path taken is in the
%    direction of pose initialization. the visualization of path taken is
%    shown in Fig. . the correlation with of path taken with initial pose
%    is shown in Fig. . The correlation with shortest path is shown in
%    Fig. . Hence, the algorithm instead of shortest path planning takes the
%    the direction in which it is initialized.
A square map is the simplest possible map with two paths to the goal.
We evaluate the algorithm trained on 1000 random maps on square map.
We observe that the agent greedily moves in the direction of
initialization.
This may be because of the initial learning which is
motivated by small rewards of getting apples.
We compute the percentage of times the agent takes the shortest path over a trial of 100 episodes.
We find the agent takes the shortest path only $50.4$\% ($\pm 12.8$\%) of the times which is not better than random.
% Quantitative result taken from:
% exp-results/planning-09x09-0002/gen_stats_latest_loaded-from-training-1000_acting-on-planning-09x09-0002_vars-True_apples-0.json

\subsubsection{Wrench map and Goal map}
% 2.2 To eliminate the correlation with initial orientation, we start
%     with wrench map. The spawn point is guaranteed to be initialized in
%     the handle of the wrench. Hence, the decision point is the only
%     junction in the map where the algorithm either decides to take a
%     left or a right.
%     We find no correlation between shortest path and the direction taken. 
To eliminate the dependence with initial orientation, we evaluate the algorithm on wrench map as shown in Fig~\ref{fig:planning-qualitative}. We fix in the spawn point at the bottom of the tail so that shortest path is independent of the spawn orientation.
The decision about the shortest path is made at the junction where the agent can either chose to go left or right.
We find that the agent is taking shortest path only $32.9$\% ($\pm 25.1$\%) of the times which is again to better than random.
% Quantitative result taken from:
% exp-results/planning-09x09-0004/gen_stats_latest_loaded-from-training-1000_acting-on-planning-09x09-0004_vars-True_apples-0.json

% Goal map provides a similar independence from starting orientation
% but the penalty for choosing the wrong path is much higher than the
% wrench map. 
Similarly, goal map (Fig~\ref{fig:planning-qualitative}) provides decision point indpendent of the initial orientation, but it penalizes wrong path more than the wrench map $42.6$\% ($\pm 35.1$\%) of the times which is again no better than random.
% Quantitative result taken from:
% exp-results/planning-09x09-0006/gen_stats_latest_loaded-from-training-1000_acting-on-planning-09x09-0006_vars-True_apples-0.json
% 

%\subsubsection{Qualitative results}
\input{fig-planning-qualitative}

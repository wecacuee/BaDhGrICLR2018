Since deep reinforcement learning algorithms need millions of iterations to train, in the absence of thousands of robotic replicas like \cite{LePaKrISER2017}, we evaluate the algorithms on a simulated environment.
We use the same game engine as \cite{MiPaViICLR2017}, called Deepmind Lab (\cite{BeLeTeARXIV2016}).
The game is setup such that an agent is placed within a randomly generated maze containing a \emph{goal} at a particular location.
On reaching the goal, the agent \emph{re-spawns} within the same maze while the goal location remains unchanged. 
Following \cite{MiPaViICLR2017}, we scatter the maze with randomly placed smaller apple rewards (+1) to encourage initial explorations and assign the goal a reward of +10.
The agent is tasked to find the goal as many times as possible within a fixed amount of time (1200 steps for our experiments), re-spawning within the maze, either statically or randomly, each time it reaches the goal.

Unlike \cite{MiPaViICLR2017}, we include a small wall penalty (-0.2) that pushes the agent away from the wall.
The wall penalty is useful to prevent agents from moving along the walls, thereby discarding vision input for exploration.
We also use a discrete 4-action space (move forward/backward, rotate left/right)which is different from the 8-action space one used by \cite{MiPaViICLR2017}.
A smaller action space helps the algorithm train faster while achieving similar reward values.

We generate 1100 random maps using depth-first search based maze generation methods.
More information on maze generation can be found in the appendix. 
Of the first 1000 maps, 10 are randomly selected for our static-map experiments (Fig. \ref{fig:environments}). For our unseen map experiments, agents are trained on increasing subsets of the first 1000 maps and tested on the remaining 100.
Unlike \cite{MiPaViICLR2017} and similar to \cite{ChLaSaNIPS2016}, we use randomly textured walls in our mazes so that the policies learned are texture-independent.


\input{grid-2d-dmlab-figure}
\input{intro-drl-nav-challenge}

\subsection{Evaluation Metrics}

We evaluate the algorithms in terms of three metrics: rewards, \emph{\LatencyOneGtOne{}} and \emph{\DistanceInefficiency{}}.
Following \cite{MiPaViICLR2017}, we report \emph{\LatencyOneGtOne{}}, a ratio of the time taken to hit the goal for the first time (exploration time) versus the average amount of time taken to hit goal subsequently (exploitation time).
The metric is a measure of how efficiently the agent exploits map information to find a shorter path once the goal location is known. 
If this ratio is greater than 1, the agent is doing better than random exploration and the higher the value, the better its map-exploitation ability.
Note that the metric is meaningful only when the goal location is unknown at evaluation time.


\emph{\DistanceInefficiency{}} is defined to be the ratio of the total distance traveled by the agent versus the sum of approximate shortest distances to the goal from each spawn point. The metric also disregards goals found during exploitation time as the agent must first find the goal before it can traverse the shortest path to it.
Note that the shortest distance between the spawn and goal locations is computed in the top-down block world perspective and hence is only an approximation.

While the \LatencyOneGtOne{} measures the factor by which planned path to the goal is shorter than the exploration path, the \DistanceInefficiency{} measures the length of this path with respect to the shortest possible path. 

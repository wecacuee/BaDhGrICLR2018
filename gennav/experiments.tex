%VD: a subheading immediately after a headng should be avoided
%\subsection{Setup}
Since deep reinforcement learning algorithms need millions of iterations to train, in the absence of thousands of robotic replicas like \cite{LePaKrISER2017}, we evaluate the algorithms on a simulated environment.
We use the same game engine as used by \cite{MiPaViICLR2017}, called Deepmind Lab \cite{BeLeTeARXIV2016}.
The game is setup such that an agent is placed within a randomly generated maze containing a \emph{goal} at a particular location.
On reaching the goal, the agent \emph{respawns} within the same maze while the goal location remains unchanged. 
Following \cite{MiPaViICLR2017}, we scatter the maze with randomly placed smaller apple rewards (+1) to encourage initial explorations and assign the goal a reward of 10.
The agents are tasked with maximizing possible reward over the course of episodes of fixed time lengths and thus must learn to repeatedly find the goal within the maze as often as possible to achieve this objective.

Unlike \cite{MiPaViICLR2017}, we include a small wall penalty (-0.2) that pushes the agent away from the wall.
% VD: we will need to show experiments, if we claim that.
%We find that this penalty is useful to prevent agents from picking up scattered rewards while moving backwards that in turn lead to local-optima based backward-moving agents that discard vision all together in their explorations.
% Reward formulation
%The setup of game is such that it enables evaluating the algorithm for optimal path planning as well as enable dividing experiments into exploration and exploitation stages.
%We introduce a metric that evaluates how well the agent exploits the information gained during exploration before finding the goal for the first time.
% The optimal way of finding the goal in fastest way is to collect
% enough information for creating a map and use the map to find the shortest path from start to end location.
% However, an end-to-end navigation algorithm might find other ways of finding optimal strategies to find the goal.
Also, we use a discrete 4-action space (move forward/backward, rotate left/right)which is different from the 8-action space one used by \cite{MiPaViICLR2017}.
% VD: Do we need to show experiments for these claims?
A smaller action space helps the algorithm train faster while achieving similar reward values.
% VD: this will raise more questions like why grounding in vision is necesary
% The agent's picking up of rewards is thus explicitly grounded in its vision as the agent must rotate in the direction of motion during exploration.

We generate 1100 random maps using depth first search based maze generation methods. More information on maze generation can be found in the appendix. 
Of these 1100 maps, 1000 maps are used for training while the remaining 100 are used for testing.
Unlike \cite{MiPaViICLR2017} and similar to \cite{ChLaSaNIPS2016}, we use randomly textured walls in our mazes so that the policies learned are texture-independent.

% VD: Let's describe this terminology in the descriptions of tasks
% Througout our experiments we refer to goal locations, spawn locations and maps as static or random.
% When static is used for spawn and goal location, they described positions that remain fixed through all episodes during both the training and the testing.
% When random is used, the spawn location changes randomly throughout the duration of the episode for both training and testing. When used for the goal, random means that its locations changes between episodes for training and testing. During the course of an episode, however, a random goal remains in the same position allowing for agents that exploit map information to find it repeatedly and thus maximize reward.
% In the context of maps, statics maps imply that the same map is used for training and testing. Random means that agents are simulataneously trained on multiple maps and tested on previously unseen ones.

\input{fig-latency-goal-reward}
\input{grid-2d-dmlab-figure}

\input{intro-drl-nav-challenge}


%We evaluate the Nav-A3C\cite{MiPaViICLR2017} algorithm on randomly chosen ten maps with increasing difficulty.
%The results of our experiments are shown in Fig~\ref{fig:latency-goal-reward}.
%We change the randomness of either the spawn point or the goal location.
%We note that the increasing randomness gradually reduce the maximum reward achived and the number of times the goal is reached. As expected, the variance in reward and number of times goals is reached also increases with randomless.
%

%\subsection{Evaluation Metrics}
%We report three evaluation metrics in all our experiments: reward, average number of goal hits and \LatencyOneGtOne{}. We run evaluations on a map for 100 episodes and take the mean and standard deviation of the rewards per episode and number of goal hits per episode. Even though reward includes rewards due to apples and negative rewards due to wall penalities, the reward due to goal dominates dominates the reward metric making reward approximately ten times the average goal hits.
%
%The \LatencyOneGtOne{} is defined as the ratio of time take to find the goal first time to the average time take to find goal thereafter.
%Note that whenever the goal location is ``Random'' (different in each episode but same for the episode), untill the goal is found for the first time, the agent is just exploring the map.
%After the first goal hit, the agent should be able to exploit the location of the goal to find the goal in shorter times.
%Hence a \LatencyOneGtOne{} value greater than one indicates that the algorithm is able to successfully exploit information gathered during goal finding exploration.
%

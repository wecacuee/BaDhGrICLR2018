% 0. Baseline models
% 1. Blinding is easy enough to understand
% 2. The doubling of the action space - that's the brilliance
%

\subsection{Replicating Learning to navigate}

Since \cite{MiPaViICLR2017} did not release their source code, so we implemented their pipeline starting from \texttt{universe-start-agent}\cite{OpenAI2017UniverseStarterAgent}.

To make sure that we replicate \cite{MiPaViICLR2017} results, we setup the experiment as similar to what was described in the paper.
But unlike, \cite{MiPaViICLR2017} instead of hand picking the maps to run experiments on, we chose to run the experiments on randomly generated maps.
We create a set of randomly generated maps by a textbook depth first search based algorithm with randomly introduced loops.
\hl{Based on results chose one}
In our experiments, we found that we do not reach the latency scores of \cite{MiPaViICLR2017} but we reach close enough.
\hl{OR}
Results in Table~\ref{tab:TODO} establish conclusively that the NavA3C agents replicated by us do perform at least as well as the agents .
%%%%%
Note, that we report standard deviation over latency metric which indicates that the variation between maps and algorithm is not significant enough to overcome the variation within the maps.
This shows that the experiments in \cite{MiPaViICLR2017} were not strong enough to support the hypothesis of learning to navigate.

\begin{table}[h]
    \label{sample-table}
    \begin{center}
        \begin{tabular}{llll}
          \toprule
          Training maps & Reward & Latency 1:$>1$ & Goals \\
          \midrule
          1 & 41.01 $\pm$  16.78 & 152.3:145.3 $\pm$ 118.88:83.95 & 3.48 $\pm$ 1.65 \\
          10 & 30.89 $\pm$  20.87 & 154.05:156.86 $\pm$ 140.03:98.26 & 2.47 $\pm$ 1.72 \\
          1000 & 40.27 $\pm$  20.47 & 148.20:148.12 $\pm$ 130.45:95.28 & 3.14 $\pm$ 1.89 \\
          \bottomrule
        \end{tabular}
    \end{center}
    \caption{We achieve comparable results when we test and train on the same experiments.}
\end{table}

\subsection{Training on multiple maps}
Can we generalize the performance of the agent by training on multiple maps? The answer turns out to be no.

\subsection{Evaluation on unseen maps}
What if we test on unseen maps, will the agents be able to tranfer their strategies onto unseen maps? The answer turns out to be no.

\subsection{Do the agents learn a frame to action mapping and repeat it?}
NO
\hl{Show qualitative and quantitative results to address this hypothesis}

\subsection{Do the agents memorize the path and repeat it?}
NO
\hl{Show qualitative and quantitative results to address this hypothesis}

\subsection{Are these agent doing better path planning than random?}
We evaluate the trained agents on 1000 random maps on very simple maps which have only two possible paths to reach the goal -- one shorter than the other. We evaluate the agent whether it does better than a random agent.
It does not.

\subsection{DeepmindLab navigation challenge}
We propose a 5-stage benchmark for reinforcement learning based methods to address the navigation problem.
While there already exists optimal algorithms to find shortest path between two points and perform optimal navigation between given set of points in a given map, the advantage of Deep reinforcement learning comes from it's ability to extract required features from the input images.
Thus we need to either integrate existing path planning and mapping methods with deep learning methods to perform end-to-end training or extend deep learning methods to learn path planning and mapping.
As we have shown this is going to be hard.
\begin{description}
  \item[Evaluate on training map with fixed goal location]
    This is the textbook version of reinforcement learning problem,
    especially in gridworld. This problem is easier because the policy
    is stationary at the train time and does not changes during
    testing.
    We propose the evaluation metric to be : the time taken to hit the goal once. We show the results on this benchmark in Table~\ref{tab:TODO-RUNEXP}.
    We compare the results against optimal planning methods which take the support of external map input.
  \item [Evaluate on training maps with random goal location]
    This make the problem harder becuase the policy function needs to depend up on the goal location.
    So instead of learning a static policy function the learning algorithm needs to learn a function that returns a different policy for different goal locations.
    This can be evaluated into ways, (a) providing the goal location as an external input to the method or (b) let the agent explore the map randomly and ask the agent to return to goal location after first round of exploration.
    We chose method (b) and show our results on this in Table~\ref{tab:TODO}.
    We compare the method against random systematic explorers, where the agent always moves forward and takes right turns on all junctions.
  \item [Evaluate on unseen maps]
    Any proposed algorithms on navigation problems, should be evaluated on unseen maps.
    Since this is a hard problem, we should compare it against random systematic explorers and should be analyzed to understand the kind of approach the learning algorithm is using.
    \begin{description}
      \item[Remember and repeat]
        We propose a series of maps, goal and spawn locations where the optimal performance can be achieved by repeating the action sequence that lead to the goal for the first time.
      \item[Frame to action mapping]
        We propose a series of maps, goal and spawn locations where the optimal performance can be achieved by learning a frame to action greedy mapping but not just by remember and repeat.
      \item[Full path planning]
        We propose a series of maps, goal and spawn locations, where the optimal performance cannot be achieved unless full path planning is done.
    \end{description}
\end{description}

\begin{figure}
 \vspace{-3em}%
\includegraphics[width=0.5\columnwidth]{images/plot_reward_3D-1000.pdf}%
\includegraphics[width=0.5\columnwidth]{images/plot_probability_3D-1000.pdf}%
\vspace{-1em}%
\caption{Mean reward while tested on 100 unseen maps, while being trained on different number of training maps. Note that while training on 1000 maps eventually achieves high reward, it is only higher mean reward (44.2), training on 1 map hits the maximum (31) much faster.}
\label{fig:plot_reward_on_testing}
\end{figure}


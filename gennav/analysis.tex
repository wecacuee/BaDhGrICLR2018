\input{fig-latency-goal-reward}
\input{grid-2d-dmlab-figure}
The results for DRL Navigation challenge are shown in Fig~\ref{fig:latency-goal-reward}.
We note that when the goal is static then rewards are consistently higher as compared to random goal while static spawn location and random spawn location are roughly close to each other within bounds of uncertainty. As expected, switching each variable from static to random increases the standard deviation on the results.
  From the \LatencyOneGtOne{} results we note that the current state of art algorithms do well when trained and tested on the same map but fail to generalize to new maps when evaluated on ability to exploit the information about goal location.
  Also note that \LatencyOneGtOne{} metric for cases of static goals is expected to be close to one because the location of goal is learned at train time.

\subsection{Evaluation on unseen maps}
The results for training on $N$ maps, where $N \in \{10, 100, 500, 1000\}$, and testing on 100 unseen maps are shown in Fig~\ref{fig:num-training-maps}. We observe that their is significant jump of average reward and average goal hits when number of training maps is increased from 10 to 100 but no significant increase when number of training maps are increased from 100 to 500 to 1000.
This is due to the fact that the bug exploration strategy learnt by the algorithm,
is learnt with enough variation in 100 maps and training on additional maps does not add to the learnt strategy.

\subsection{Effect of apples and texture}
We evaluate the effect of apples and texture during evaluation time in Fig~\ref{fig:num-training-maps}.
We train the algorithm on randomly chosen training maps with random texture and evaluate them no maps with and without random texture and also on maps with and without apples. When the apples are present, we place the apples with probability 0.25 in each block of the map.
We find that the algorithm, being trained on random textures and random placement of apples, is robust to presence or absence of textures and apples.


\subsection{Qualitative evaluation on simple maps}
% 1. We test the trained algorithms on simple maps.
To evaluate what strategies that algorithm is employing to reach the
goal we evaluate the algorithm on very simple maps where there are only two
paths to reach the goal. The qualitative results for the evaluation are shown
in Fig~\ref{fig:planning-qualitative}.

\begin{figure}%
  \includegraphics[width=\linewidth]{images/plot_ntrain_summary.pdf}%
  \caption{Plots showing the effect of number of training maps with random texture (Rnd Texture) and presence of apples (With apples), when evaluated on unseen maps. We note that although the difference between mean metrics is negligible as compared to standard deviation of the metrics. Hence we say that the effect of apples or textures can be ignored.
  The only clear trend is apparent \LatencyOneGtOne{} metric which suggest that random texture along without apples is advantageous in exploiting goal location while finding the goal second time on-wards.}
  \label{fig:num-training-maps}
\end{figure}

\paragraph{Square map}
% 3. In the square map, we find that the path taken is in the
%    direction of pose initialization. the visualization of path taken is
%    shown in Fig. . the correlation with of path taken with initial pose
%    is shown in Fig. . The correlation with shortest path is shown in
%    Fig. . Hence, the algorithm instead of shortest path planning takes the
%    the direction in which it is initialized.
A Square map (Fig~\ref{fig:planning-qualitative}) is the simplest possible map with two paths to the goal.
We evaluate the algorithm trained on 1000 random maps on square map.
We observe that the agent greedily moves in the direction of
initialization.
This may be because of the initial learning which is
motivated by small rewards of getting apples.
We compute the percentage of times the agent takes the shortest path over a trial of 100 episodes.
We find the agent takes the shortest path only $50.4$\% ($\pm 12.8$\%) of the times, no better than random.
% Quantitative result taken from:
% exp-results/planning-09x09-0002/gen_stats_latest_loaded-from-training-1000_acting-on-planning-09x09-0002_vars-True_apples-0.json

\paragraph{Wrench map}
% 2.2 To eliminate the correlation with initial orientation, we start
%     with wrench map. The spawn point is guaranteed to be initialized in
%     the handle of the wrench. Hence, the decision point is the only
%     junction in the map where the algorithm either decides to take a
%     left or a right.
%     We find no correlation between shortest path and the direction taken. 
To eliminate the dependency on initial orientation, we evaluate the algorithm on Wrench map as shown in Fig~\ref{fig:planning-qualitative}. We fix in the spawn point at the bottom of the tail so that shortest path is independent of the spawn orientation.
The decision about the shortest path is made at the junction where the agent can either chose to go left or right.
We find that the agent is taking shortest path only $32.9$\% ($\pm 25.1$\%) of the times which is again to better than random.
% Quantitative result taken from:
% exp-results/planning-09x09-0004/gen_stats_latest_loaded-from-training-1000_acting-on-planning-09x09-0004_vars-True_apples-0.json

% Goal map provides a similar independence from starting orientation
% but the penalty for choosing the wrong path is much higher than the
% wrench map. 
\paragraph{Goal map}
Similarly to the wrench map, the goal map (Fig~\ref{fig:planning-qualitative}) provides decision point independent of the initial orientation, but it penalizes wrong path more than the wrench map $42.6$\% ($\pm 35.1$\%) of the times which is again no better than random.
% Quantitative result taken from:
% exp-results/planning-09x09-0006/gen_stats_latest_loaded-from-training-1000_acting-on-planning-09x09-0006_vars-True_apples-0.json
% 

%\subsubsection{Qualitative results}
\input{fig-planning-qualitative}

\subsection{Computing Attention Maps}
We use the normalized sum of absolute gradient of the loss with respect to the input image as a proxy for attention in the image.
The gradients are normalized for each image so that the maximum gradient is one. The attention values are then used as a soft mask on the image to create the visualization as shown in Fig~\ref{fig:attention}

We observe that the attention is uniformly distributed on the image when the agent spawns. The attention narrows down to a few pixels in the center when the agent is navigating through the corridor. It spreads to the entire image around turns and junctions. The attention also pays close attention to important objects like goal, apples and unique decals.

\begin{figure}
\includegraphics[width=\textwidth,trim=0 0 0 336pt,clip]{./exp-results/training-09x09-0127-on-0127.png}\vspace{1ex}\\
%
\includegraphics[width=\textwidth,trim=0 0 0 336pt,clip]{./exp-results/training-1000-on-0127.png}%
\caption{Visualizing attention for two sequences. The first two rows show the sequin when the model is trained on and evaluated on the same map. The last two rows shows the sequence for a model trained on 1000 maps and evaluated on one of the maps. We observe that the attention is uniformly distributed on the image when the agent spawns. The attention narrows down few pixels in the center when the agent is navigating through the corridor. It spreads to the entire image around turns and junctions. The attention also pays close attention to important objects like goal, apples and unique decals.}
\label{fig:attention}
\end{figure}


%Are we close to replacing the classic mapping and path-planning algorithms with DRL algorithms?
%
%As shown by the results in Fig~\ref{fig:latency-goal-reward}, the state of art DRL algorithm works well if trained and tested on the same map (static map), but performance reaches to the level of simple bug exploration like algorithms if training and testing are on different maps.




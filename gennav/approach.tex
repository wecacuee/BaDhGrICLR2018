% 0. Describe baseline models
% 1. Introduce blinding in a curriculum and finetuning manner
% 2. Introduce the concept of doubling the action space
% 3. Introduce the potential applicablity of this blinding to querying
% the surrounding states of the agent.

\subsection{Baseline Models}
We utilize the model presented in Mirowski \etal as our baseline model. The agent's egocentric view is fed in to a deep network that in turn converts it to an action to take within the environment. Due to the POMDP nature of the problem, memory is provided to the agent via a set of stacked LSTMs so that it can learn to assign contextual importances to past actions and observations. As mentioned in the paper, we provide our agents will the auxiliary loss signals of depth prediction and loop closure to improve convergence speeds. Our agents are trained on the same environments utilized in the original paper  and display comparable results. We additionally train on our agents on newer random environments of different dimensions coupled with several more wall texture varieties so as to provide more quantitative evaluations of the ability of these agents in more diverse environments.

\subsection{Blinding}
We incrementally blind these agents so as to guage the amount of information that is actually required by agents to perform navigation in the contexts of these environments. Intuitively, we expect agents thus trained to perform better long-term planning due to the potential unreliability of future observations at any given point. We experiment with two main types of blinding.  

\subsubsection{Curriculum Blinding}
In currulum blinding, agents are simultaneously asked to learn to navigate in conjunction with receiving increasingly "blind" data. The agent is tasked with learning a harder combined task in the hope that it more readily learns how its actions can affect long term rewards gained instead of relying on receiving the corresponding frames at every single point of time. Blinding is linearly increased from 0 to 100 over the course of each trial. 

\subsubsection{Fine-tuning}
In the fine-tuning approach, baseline agents are first trained on the map using the standard A3C approach. Agents are then "fine-tuned" by being fed increasingly blind data where the blindness again climbs linearly over the training period. The hope is that the agent learns to augment its already learned navigation strategies with improve versions of looking in to the future due to the blindness etc etc. 

\subsection{Blinding: Self-Supervised Curriculum Training}
In both sets of blindfolding experiments described thus far, the blinding is forced upon the robotduring training time. Every maze, however, possesses different degrees of difficulty in their different parts. For example, navigating a single corridor blind is a much easier task than that of navigating cross roads.

Based on this idea, we introduce BLINC, wherein agents are incentivized via small additional reward signals to 

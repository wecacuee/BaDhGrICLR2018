Our problem formulation is based upon the work of \cite{MiPaViICLR2017}. We summarize the technical setup here for completeness.
We refer the reader to \cite{MnBaMiICML2016,MiPaViICLR2017} for more details of the setup.

The problem of navigation is formulated as an interaction between an environment and an agent.
At time $t$ the agent takes an action $\actt \in \Action$ and observes observation $\obst \in \Obs$ along with a reward $\rewt \in \R$.
We assume the environment to be a Partially Observable Markov Decision Process (POMDP).
In a POMDP, the future state of the environment, $\statetn \in \State$, is conditionally independent of all the past states, $\statehist$, given the current state $\statet$. It is further assumed that
$\obst$ and $\rewt$ are independent of previous states given current state $\statet$ and last action $\acttp$.
Formally, a POMDP is defined as a six tuple $(\Obs, \ObsFunc, \State, \Action, \Trans, \Rew)$ that is composed of an observation space $\Obs$, an observation function $\ObsFuncFull$, a state space $\State$, an action space $\Action$, a transition function $\TransFull$ and a reward function $\RewFull$.
For our problem setup, the observation space $\Obs$ is the space of an encoded feature vector that is generated from input image along with previous action and reward.
Action space $\Action$ contains four actions: rotate left, rotate right, move forward and move backward and reward function $\Rew$ is defined for each experiment so that reaching the goal leads to high reward with auxiliary reward to encourage certain kind of behavior.

For DRL algorithms, the state space $\State$ is not hand tuned, but it is modeled as a vector of floats.
Also, instead of modeling observation function $\ObsFuncFull$ and $\TransFull$, a combined transition function $\TransObsFull$ is modeled such that it estimates the next state $\statetn$ directly considering previous observation as well as reward into account.
For policy-based DRL a policy function $\piDef$ and a value function $\ValueDef$ are also modeled.
All three functions $\TransObs$, $\pit$, $\Valuet$ share most of the parameters in a way such that $\theta_T \subseteq \theta_{\pi} \cap \theta_\Value$

%Since we aim to estimate $\ObsFuncInv$ and $\Trans$ from experience, we formulate the experience as observation tuples divided into episodes of fixed length $E$.
%Each episode experience contains $E$ tuples with observation, action and corresponding reward $D_E = \{(\obs_0, \act_0, r_0), \dots, (\obs_E, \act_E, r_E)\}$. After each episode the state $\state_t$ is reset to all zeros and another data sequence is collected. Let the collected dataset be $D_N = \{
% JJC: TODO: Our objective or The DRL objective
The DRL objective is to estimate unknown weights $\theta = \theta_T \cup \theta_\pi \cup \theta_V$ that maximizes the expected future reward, $R_t = \sum_{k=t}^{t_{end} - t} \gamma^{k-t} r_k$, where $\gamma$ is the discount factor,
%
\begin{align}
\theta^* = \arg\max_{\theta} \E[R_t] \,.
\end{align}
%
% need \graphicspath{{images/}}
%\def\svgwidth{0.25\columnwidth}%
\begin{figure}%
%\input{images/a3c-as-pomdp.pdf_tex}%
%\def\svgwidth{0.25\columnwidth}%
%\input{images/a3c-as-nn.pdf_tex}%
\begin{center}
\scalebox{1.3}{\input{fig-nav-a3c}}%
\end{center}
%\input{fig-cnn-enc}\hspace{-1ex}%
%\input{fig-fc-enc}%
\caption{Modified \NavAiiiCDiDiiL{} (\cite{MiPaViICLR2017}) architecture.
The architecture is has three inputs the current image $I_t$ and previous action $\acttp$ and previous reward $\rewtp$.
As shown by \cite{MiPaViICLR2017}, the architecture improves upon vanilla A3C architecture by using auxiliary outputs of loop-closure signal $L$ and predicted depth $D_1$ and $D_2$.
Since we use a smaller action space than \cite{MiPaViICLR2017} and our agent moves with constant velocity, we do not use velocity at previous time step as input signal.}
\label{fig:architectures}
\end{figure}

\paragraph{Asynchronous Advantage Actor-Critic}
\def\charelig{\nabla_{\theta_\pi}\ln \pit(\acttn; \theta_\pi)}
% There are many different variations of RL. There are many different RL algorithms: value-based methods like Q-learning and SARSA and policy-based method like actor-critic.
In this paper we use the policy-based method called Asynchronous Advantage Actor-Critic (A3C) (\cite{MnBaMiICML2016}) that allows weight updates to happen asynchronously in a multi-threaded environment.
It works by keeping a ``shared and slowly changing copy of target network'' that is updated every few iterations by accumulated gradients in each of the threads.
The gradients are never applied to the local copy of the weights, but the local copy of weights is periodically synced from the shared copy of target weights.
The gradient for the weight update is proportional to the product of \emph{advantage}, $R_t - \Value_t(\theta_\Value)$, and \emph{characteristic eligibility}, $\charelig$ (\cite{WiML1992}), updating the weights according to the following update equations
\begin{align}
  \theta_\pi &\leftarrow \theta_\pi
  + \sum_{t \in \text{episode}}\alpha_\pi \charelig (R_t - \Value_t(\theta_\Value))
  \\
  \theta_\Value &\leftarrow \theta_\Value
  + \sum_{t \in \text{episode}} \alpha_\Value \frac{\partial (R_t - \Value_t(\theta_\Value))^2}
                  {\partial\theta_\Value}
                  \, .
\end{align}

For more details of the A3C algorithm we refer the reader to \cite{MnBaMiICML2016}.
\paragraph{\NavAiiiCDiDiiL{}}
In this work we use the \NavAiiiCDiDiiL{} architecture as proposed by \cite{MiPaViICLR2017} which builds modifying the network architecture to have two LSTMs and with auxiliary outputs of depth predictions along with loop-closure predictions.
The schematic of the architecture is shown in Fig~\ref{fig:architectures}.
The architecture has three inputs the current image $I_t$ and previous action $\acttp$ and previous reward $\rewtp$.
As shown by \cite{MiPaViICLR2017}, the architecture improves upon vanilla A3C architecture by optimizing predictions for the auxiliary outputs of loop closure signal $L$ and predicted depth $D_1$ and $D_2$.
Since we use a smaller action space than \cite{MiPaViICLR2017} and our agent moves with constant velocity, we do not use velocity at the previous time step as an input signal.


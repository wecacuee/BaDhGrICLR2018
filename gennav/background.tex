Our problem formulation is based on the work of \cite{MiPaViICLR2017}. 
For completeness, we summarize the technical setup here.
For additional details regarding the setup, we refer the reader to \cite{MnBaMiICML2016,MiPaViICLR2017}.

The problem of navigation is formulated as an interaction between an environment and an agent.
At time $t$ the agent takes an action $\actt \in \Action$ and observes observation $\obst \in \Obs$ along with a reward $\rewt \in \R$.
We assume the environment to be a Partially Observable Markov Decision Process (POMDP).
In a POMDP, the future state of the environment, $\statetn \in \State$, is conditionally independent of all the past states, $\statehist$, given the current state $\statet$. It is further assumed that
$\obst$ and $\rewt$ are independent of previous states given current state $\statet$ and last action $\acttp$.
Formally, a POMDP is defined as a six tuple $(\Obs, \ObsFunc, \State, \Action, \Trans, \Rew)$ that is composed of an observation space $\Obs$, an observation function $\ObsFuncFull$, a state space $\State$, an action space $\Action$, a transition function $\TransFull$ and a reward function $\RewFull$.
For our problem setup, the observation space $\Obs$ is the space of an encoded feature vector that is generated from input image along with previous action and reward.
Action space $\Action$ contains four actions: rotate left, rotate right, move forward and move backward and reward function $\Rew$ is defined for each experiment so that reaching the goal location leads to high reward with auxiliary rewards to encourage certain kinds of behavior.

For DRL algorithms, the state space $\State$ is not hand tuned, but it is modeled as a vector of floats.
Additionally, instead of modeling observation function $\ObsFuncFull$ and $\TransFull$, a combined transition function $\TransObsFull$ is modeled to estimate the next state $\statetn$ and directly take previous observation and reward into account.
For policy-based DRL a policy function $\piDef$ and a value function $\ValueDef$ are also modeled.
All three functions $\TransObs$, $\pit$ and $\Valuet$ share most of the parameters such that $\theta_T \subseteq \theta_{\pi} \cap \theta_\Value$

%Since we aim to estimate $\ObsFuncInv$ and $\Trans$ from experience, we formulate the experience as observation tuples divided into episodes of fixed length $E$.
%Each episode experience contains $E$ tuples with observation, action and corresponding reward $D_E = \{(\obs_0, \act_0, r_0), \dots, (\obs_E, \act_E, r_E)\}$. After each episode the state $\state_t$ is reset to all zeros and another data sequence is collected. Let the collected dataset be $D_N = \{
% JJC: TODO: Our objective or The DRL objective
The DRL objective is to estimate unknown weights $\theta = \theta_T \cup \theta_\pi \cup \theta_V$ that maximizes the expected future reward $R_t = \sum_{k=t}^{t_{end} - t} \gamma^{k-t} r_k$ (where $\gamma$ is the discount factor) and is expressed as
%
\begin{align}
\theta^* = \arg\max_{\theta} \E[R_t] \,.
\end{align}
%
%\input{fig-nav-a3c-horiz-figure}

\paragraph{Asynchronous Advantage Actor-Critic}
\def\charelig{\nabla_{\theta_\pi}\ln \pit(\acttn; \theta_\pi)}
% There are many different variations of RL. There are many different RL algorithms: value-based methods like Q-learning and SARSA and policy-based method like actor-critic.
In this work, we use the policy-based method called Asynchronous Advantage Actor-Critic (A3C) (\cite{MnBaMiICML2016}), which allows weight updates to happen asynchronously in a multi-threaded environment.
It works by keeping a ``shared and slowly changing copy of target network'' that is updated every few iterations by accumulated gradients in each of the threads.
The gradients are never applied to the local copy of the weights; instead, a local copy of weights is periodically synced from the shared copy of target weights.
The gradient for the weight update is proportional to the product of \emph{advantage}, $R_t - \Value_t(\theta_\Value)$, and \emph{characteristic eligibility}, $\charelig$ (\cite{WiML1992}), which update the weights as
\begin{align}
  \theta_\pi &\leftarrow \theta_\pi
  + \sum_{t \in \text{episode}}\alpha_\pi \nabla_{\theta_\pi}\ln \pit (R_t - \Value_t(\theta_\Value))
  \\
  \theta_\Value &\leftarrow \theta_\Value
  + \sum_{t \in \text{episode}} \alpha_\Value \frac{\partial (R_t - \Value_t(\theta_\Value))^2}
                  {\partial\theta_\Value}
                  \, .
\end{align}

For additional details of the A3C algorithm, we refer the reader to \cite{MnBaMiICML2016}.
\input{fig-nav-a3c-vert-figure}
\paragraph{\NavAiiiCDiDiiL{}}
In this work, we use the \NavAiiiCDiDiiL{} architecture as proposed by \cite{MiPaViICLR2017}, which builds modifying the network architecture to have two LSTMs and with auxiliary outputs of depth predictions along with loop-closure predictions. % TODO "which builds modifying the network architecture..." this doesn't make sense to me, should be corrected.
The schematic of the architecture is shown in Fig~\ref{fig:architectures}.
The architecture has three inputs: the current image $I_t$, previous action $\acttp$ and previous reward $\rewtp$.
As shown by \cite{MiPaViICLR2017}, the architecture improves upon vanilla A3C architecture by optimizing predictions for the auxiliary outputs of loop closure signal $L$ and predicted depth $D_1$ and $D_2$.
Since we use a smaller action space than \cite{MiPaViICLR2017} and our agent moves with constant velocity, we do not use velocity at the previous time step as an input signal.


\paragraph{Localization and mapping}
Localization and mapping for navigation is a classic problem in mobile robotics and sensing.
\cite{SmChIJRR1986} introduced the idea of propagating spatial uncertainty for robot localization while mapping, and \cite{ElCOMPUTER1980} popularized Occupancy Grids.
In the three decades since these seminal works, the field has exploded with hundreds of algorithms for many types of sensors (e.g., cameras, laser scanners, sonars and depth sensors).
These algorithms vary by how much detail is captured in their respective maps. For example, topological maps, like \cite{KuCOGSCI1978}, aim to capture as little information as possible while occupancy grid maps, \cite{ElCOMPUTER1980}, aim to capture metrically accurate maps in resolutions dependent upon the navigation task.

All these approaches require significant hand-tuning for the environment, sensor types and navigation constraints of the hardware.
In contrast, end-to-end navigation algorithms optimize the detail of map storage based on the navigation task at hand, which makes them worth exploring.

\paragraph{Deep reinforcement learning}
DRL gained prominence recently when used by \cite{MnKaSiNIPSDLW2013,MnKaSiNATURE2015} to train agents that outperform humans on Atari games; agents that trained using only the games' visual output.
More recently, DRL has been applied to end-to-end navigation (\cite{OhChSiICML2016,MiPaViICLR2017,ChLaSaNIPS2016}).
It is common for agents to be trained and tested on the same maps with the only variation being the agent's initial spawn point and the map's goal location (\cite{MiPaViICLR2017,ZhMoKoICRA2017,KuSaGaAPA2016}). 

In contrast, \cite{OhChSiICML2016} test their algorithm on random unseen maps, but their agents are trained to choose between multiple potential goal locations based on past observations.
The episodes end when the agent collects the goal, so there is no requirement for the algorithm to store map information during their exploration.
Thus, their agents decide to avoid a goal of a particular color while seeking other colors rather than remembering the path to the goal.
\cite{ChLaSaNIPS2016} test their method on unseen maps in the VizDoom environment, but only vary the maps with unseen textures. Thus, their agents are texture invariant, but train and test on maps with the same geometric structure.
%
% TODO: Add more literature. Who has cited \cite{MiPaViICLR2017} in the last one year and address the latest developments.

In this work, we extend the study of these methods in a more comprehensive set of experiments to address the question of whether DRL-based agents remember enough information to obviate mapping algorithms or may in fact need to be augmented with mapping for further progress.
 

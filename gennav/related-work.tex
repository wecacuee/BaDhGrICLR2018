\paragraph{Localization and mapping}
Robotic localization and mapping for navigation is a classic problem in  mobile robotics and sensing.
\cite{SmChIJRR1986} introduced the idea of propagating spatial uncertainty for robot localization while mapping and Elfes popularized Occupancy Grids~\cite{ElCOMPUTER1980}.
In the last three decades since these seminal works, the field has exploded with hundreds of algorithms released for multiple kinds of sensors such as cameras, laser scanners, sonars and depth sensors.
Several of these algorithms have utilized varying levels of detail in the building of their map-models ranging from low-levelled ones such as topological maps \cite{KuCOGSCI1978} and sparse reconsctruction methods to high-level ones like occupancy grids and metrically accurate dense reconstructions. 

All these approaches require significant hand-tuning and design for deployment in different kinds of environments and sensor types. The level of detail utilized in the creation of the  maps also needs to be decided before hand irrespective of the application and hence the amount of information stored is not optimized for the task at hand.

\paragraph{Deep reinforcement learning}
Deep reinforcement learning (DRL) was originally conceived in 1995 \cite{TeACM1995}. 
The field gained prominence again when \cite{MnKaSiNIPSDLW2013,MnKaSiNATURE2015} used DRL to create agents that outperformed humans on Atari games while being purely trained on the games visual outputs.
Subsequently, the field has been extended in several directions \cite{MnBaMiICML2016} being applied to creation of the Alpha-GO agent\cite{SiHuMaNATURE2016}, simulated platforms \cite{KaStJoNIPS2017}, real world robots \cite{LePaKrISER2017} and more recently to robotic navigation \cite{MiPaViICLR2017,OhChSiICML2016}.

The exploration into robotic navigation using DRL is a nascent topic having the potential to disrupt the fields of simultaneous localization, mapping and path planning. However literature concerning DRL based navigation usually ignores the standard machine learning practice of separating the training and testing sets thereby limiting our understanding of the generality of these methods. In fact, it is common for agents to be trained and tested on the same maps with the variation only being applied to the agent's initial spawn point and the map's goal location \cite{MiPaViICLR2017,ZhMoKoICRA2017,KuSaGaAPA2016}. 

In contrast, \cite{OhChSiICML2016} do test on random maps but their agents are trained to choose between multiple possible goal locations based on past observations. Their episodes end when the agent collects a goal and hence there is no requirement for their methods to store map information during their exploration. Thus the decisions made by these agents are to avoid a goal of particular a color and seek other colors rather than remembering the path to the goal. 

On similar lines, \cite{ChLaSaNIPS2016} test their method on unseen maps in VizDoom environment but only vary the maps with unseen textures. Thus their agents are texture invariant but continue to train and test on maps with the same geometric structure.
%
In this work, we take the study of these methods significantly farther with a thorough investigation of whether DRL-based agents remember enough information to obviate mapping algorithms or whether they do in fact need to be augmented with mapping algorithms for future progress.

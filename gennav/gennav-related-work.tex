\paragraph{Localization and mapping}
Robotic localization and mapping for navigation as a problem since the beginning of mobile robotics and sensing.
Smith and Cheeseman~\cite{SmChIJRR1986} introduced the idea of propagating spatial uncertainty for robot localization while mapping and Elfes popularized Occupancy Grids~\cite{ElCOMPUTER1980} for mapping.
In the last three decades, the field has exploded with variation of algorithms for different sensors like cameras, laser scanners, sonars, depth sensors, variation in level of detail like topological maps \cite{KuCOGSCI1978} for low level of detail to occupancy grid maps for high detail and variation in environment types like highly textured or non-textured.

All these approaches require huge amount of hand-tuning and design for adapting to different environments and sensor types. The level of detail of maps also needs to be decided before hand irrespective of the application and hence is not optimized for the application at hand.

\paragraph{Deep reinforcement learning}
Deep reinforcement learning (DRL) came back to the limelight \cite{TeACM1995} \hl{Check whether this citation should be here} with Mnih \etal~ \cite{MnKaSiNATURE2015,MnKaSiNIPSDLW2013} demonstrating that their algorithms outperform humans on Atari games. Subsequently, the DRL algorithms have been extended \cite{MnBaMiICML2016} and applied to various games \cite{SiHuMaNATURE2016}, simulated platforms \cite{KaStJoNIPS2017}, real world robots \cite{LePaKrISER2017} and more recently to robotic navigation \cite{MiPaViICLR2017,OhChSiICML2016}.

The exploration into robotic navigation using deep reinforcement learning is a nascent topic, it has potential to disrupt the fields of simultaneous localization and mapping and path planning. Also, \cite{MiPaViICLR2017} train and test on the same maps which limits our understanding of the generality of the method. In fact, it is very common to train and test on the same environments in reinforcement learning based navigation works \cite{zhu2017target,kulkarni2016deep} with the only variation being in location of goal and starting point. In contrast, \cite{OhChSiICML2016} do test on random maps but the only decision that the agent has to make is avoid a goal of particular color and seek other color rather than remembering the path to the goal. On similar lines, \cite{chaplottransfer} test their method on unseen maps in VizDoom environment but only vary the maps by unseen texture.
%
In this work, we take the study of these methods significantly farther with a thorough investigation of whether DRL-based agents remember enough information to obviate mapping algorithms or the need to be augmented with mapping algorithms.
